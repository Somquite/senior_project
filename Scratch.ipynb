{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Train Keras version:  2.7.0\n",
      "Reg Train TF version:  2.7.0\n",
      "model_dir =  ./models/\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# !pip3 uninstall PIL\n",
    "# !pip3 install --upgrade pip tensorflow\n",
    "# !pip3 install --upgrade --force-reinstall matplotlib pillow\n",
    "\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "# import cupy as cp\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "# import PIL\n",
    "# from pillow import Image\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import Input\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import *\n",
    "import tempfile\n",
    "\n",
    "print(\"Reg Train Keras version: \", keras.__version__)\n",
    "print(\"Reg Train TF version: \", tf.__version__)\n",
    "\n",
    "from ml_library.model import *\n",
    "from ml_library.utils import *\n",
    "from ml_library.config import *\n",
    "from ml_library.w_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_dir = './models/'\n",
    "#tmpdir = tempfile.mkdtemp()\n",
    "#print(\"Tmpdir = \", tmpdir)\n",
    "print(\"model_dir = \", model_dir)\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29644/234177314.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimportDatasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtraining_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./Datasets/GTSRB_Final_Training_Images.zip\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\akomm\\Github\\Jetson\\Notebooks\\ml_library\\utils.py\u001b[0m in \u001b[0;36mimportDatasets\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mTEST_FILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./Datasets/GTSRB_Final_Test_Images.zip\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;31m# ssl._create_default_https_context = ssl._create_unverified_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAINING_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Get file from URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ssl' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "importDatasets()\n",
    "\n",
    "\n",
    "# Load data\n",
    "training_file = \"./Datasets/GTSRB_Final_Training_Images.zip\"\n",
    "# testing_file = \"./Datasets/GTSRB_Final_Test_Images.zip\"\n",
    "\n",
    "train = generateTensor(training_file)\n",
    "# test = generateTensor(testing_file)\n",
    "\n",
    "x_train, y_train = split(train)\n",
    "# x_test, y_test = split(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 31367 Validation size: 7842 (0.200)\n",
      "No classes: 43\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (31367, 32, 32, 3)\n",
      "y_train shape: (31367,)\n",
      "x_val shape: (7842, 32, 32, 3)\n",
      "y_val shape: (7842,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "label_map = map_labels('signnames.csv')\n",
    "\n",
    "file_paths = len(x_train)\n",
    "from ml_library.model import *\n",
    "#Train/validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    np.asarray(x_train), np.asarray(y_train), test_size=VALIDATION_SIZE)\n",
    "\n",
    "\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_val = utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "valid_ratio = len(x_val) / file_paths\n",
    "print(\"Train size: {} Validation size: {} ({:0.3f})\".format(\n",
    "    len(x_train),\n",
    "    len(x_val),\n",
    "    valid_ratio))\n",
    "\n",
    "classes, dist = np.unique(y_train, return_counts=True)\n",
    "NUM_CLASSES = len(classes)\n",
    "print(\"No classes: {}\".format(NUM_CLASSES))\n",
    "\n",
    "plt.bar(classes, dist, align='center', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "# print(\"x_test shape:\", x_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from ml_library.model import *\n",
    "from ml_library.config import *\n",
    "\n",
    "# original_dim = 784\n",
    "original_dim = IMG_SIZE*IMG_SIZE\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "# Define our metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('val_accuracy')\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "# # Now we get a test dataset.\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "# test_dataset = test_dataset.batch(64)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(\n",
    "    1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "vae_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "vae_ds = vae_ds.shuffle(buffer_size=1024).batch(64)\n",
    "# vae_ds = vae_ds.cache().shuffle(\n",
    "#     1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "vae.compile(optimizer, loss= mse_loss_fn, metrics= loss_metric)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "# !rm - rf ./logs/fit/vae\n",
    "\n",
    "log_dir_vae = \"logs/fit/vae\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir_vae, histogram_freq=1)\n",
    "\n",
    "# vae.fit(x=x_train, y=x_train, epochs=NUM_EPOCH, validation_data=(x_val, x_val), callbacks=[tensorboard_callback], batch_size=64, verbose=1)\n",
    "\n",
    "\n",
    "# epochs = NUM_EPOCH\n",
    "\n",
    "# # Iterate over epochs.\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     for step, x_batch_train in enumerate(vae_ds):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             reconstructed = vae(x_batch_train)\n",
    "#             # Compute reconstruction loss\n",
    "#             loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "#             loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "#         grads = tape.gradient(loss, vae.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "#         loss_metric(loss)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))\n",
    "\n",
    "\n",
    "# vae.evaluate(x_val, x_val, verbose=1)\n",
    "# model_path = os.path.join(model_dir, 'normal_vae')\n",
    "# print('Saving model...')\n",
    "# vae.save(model_path)\n",
    "# # tf.saved_model.save(vae, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 16, 16, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 6, 6, 64)          9280      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 3, 3, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 3, 3, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1024)              132096    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 43)                44075     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,456,939\n",
      "Trainable params: 1,456,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "491/491 [==============================] - 220s 445ms/step - loss: 2.1923 - accuracy: 0.3427 - val_loss: 1.1729 - val_accuracy: 0.6047\n",
      "Epoch 2/5\n",
      "491/491 [==============================] - 214s 435ms/step - loss: 0.8877 - accuracy: 0.7031 - val_loss: 0.5676 - val_accuracy: 0.8141\n",
      "Epoch 3/5\n",
      "491/491 [==============================] - 214s 436ms/step - loss: 0.4819 - accuracy: 0.8420 - val_loss: 0.3115 - val_accuracy: 0.8922\n",
      "Epoch 4/5\n",
      "491/491 [==============================] - 214s 436ms/step - loss: 0.3149 - accuracy: 0.8974 - val_loss: 0.2466 - val_accuracy: 0.9219\n",
      "Epoch 5/5\n",
      "491/491 [==============================] - 213s 434ms/step - loss: 0.2209 - accuracy: 0.9291 - val_loss: 0.1892 - val_accuracy: 0.9422\n",
      "Evaluate\n",
      "123/123 [==============================] - 18s 142ms/step - loss: 0.2032 - accuracy: 0.9355\n",
      "Saving model...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\akomm\\AppData\\Local\\Temp\\tmp30k2qag3\\normal_model\\assets\n",
      "Saved model.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from ml_library.model import *\n",
    "from ml_library.config import *\n",
    "\n",
    "\n",
    "# normal_model = get_compiled_model(REIN=False)\n",
    "###################\n",
    "# https://keras.io/guides/sequential_model/ example code\n",
    "#def get_uncompiled_model(REIN=False):\n",
    "if True:\n",
    "    inputs = keras.Input(\n",
    "        shape=(IMG_SIZE, IMG_SIZE, NUM_CHANNELS), name=\"inputs\")\n",
    "    model = keras.Sequential()\n",
    "    # model.add(inputs)\n",
    "    # model.add(resize_and_rescale)\n",
    "\n",
    "    # # REIN Data Augmentation\n",
    "    # if(REIN):\n",
    "    #     model.add(data_augmentation)\n",
    "\n",
    "    model.add(Conv2D(16, 3, padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(64, 3, strides=(3, 3), padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # FC Layers w/ Dropout\n",
    "    model.add(Dense(1024, activation='gelu'))\n",
    "    model.add(Dropout(RATE))\n",
    "    model.add(Dense(1024, activation='gelu'))\n",
    "    model.add(Dropout(RATE))\n",
    "\n",
    "    model.add(Dense(NUM_CLASSES, activation=\"softmax\", name=\"predictions\"))\n",
    "\n",
    "    outputs = model.get_layer(name=\"predictions\").output\n",
    "    # build model\n",
    "    model2 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model = model2\n",
    "    #return model\n",
    "\n",
    "\n",
    "#def get_compiled_model(REIN=False):\n",
    "if True:\n",
    "    #model = get_uncompiled_model(REIN)\n",
    "\n",
    "    # model.compile(\n",
    "    #     optimizer=\"rmsprop\",\n",
    "    #     loss=\"sparse_categorical_crossentropy\",\n",
    "    #     metrics=[\"sparse_categorical_accuracy\"],\n",
    "    # )\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                      from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    #return model\n",
    "\n",
    "    normal_model = model\n",
    "##################\n",
    "log_dir_model = \"logs/fit/vae\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir_model, histogram_freq=1)\n",
    "\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "# hist_norm = normal_model.fit([vae.predict(x_train), y_train], validation_data=val_dataset,\n",
    "#                              validation_steps=10, callbacks=[tensorboard_callback], epochs=NUM_EPOCH, verbose=1)\n",
    "hist_norm = normal_model.fit(train_dataset, validation_data=val_dataset,\n",
    "                             validation_steps=10, callbacks=[tensorboard_callback], epochs=NUM_EPOCH, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluate\")\n",
    "result_normal = normal_model.evaluate(val_dataset)\n",
    "dict(zip(normal_model.metrics_names, result_normal))\n",
    "\n",
    "\n",
    "model_path = os.path.join(model_dir, 'normal_model')\n",
    "normal_model_path = model_path\n",
    "print('Saving model...')\n",
    "normal_model.save(model_path)\n",
    "# tf.saved_model.save(normal_model, model_path)\n",
    "print('Saved model.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train REIN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vae = VariationalAutoEncoder(original_dim, IMG_SIZE*2, IMG_SIZE, REIN=True)\n",
    "# vae.compile(optimizer, loss=mse_loss_fn, metrics=loss_metric)\n",
    "# vae.fit(x_train, x_train, epochs=NUM_EPOCH,  batch_size=64, verbose=1)\n",
    "# vae.evaluate(x_val, x_val, verbose=1)\n",
    "\n",
    "# model_path = os.path.join(model_dir, 'rein_vae')\n",
    "# print('Saving model...')\n",
    "# vae.save(model_path)\n",
    "# tf.saved_model.save(vae, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip REIN model for now\n",
    "if False:\n",
    "    rein_model = get_compiled_model(REIN=True)\n",
    "\n",
    "\n",
    "    # Since the dataset already takes care of batching,\n",
    "    # we don't pass a `batch_size` argument.\n",
    "    # hist_rein = rein_model.fit(vae.predict(train_dataset), validation_data=val_dataset,\n",
    "    #                            validation_steps=10, epochs=NUM_EPOCH, verbose=1)\n",
    "    hist_rein = rein_model.fit(train_dataset, validation_data=val_dataset,\n",
    "                            validation_steps=10, epochs=NUM_EPOCH, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate\")\n",
    "    result_rein = rein_model.evaluate(val_dataset)\n",
    "    dict(zip(rein_model.metrics_names, result_rein))\n",
    "\n",
    "\n",
    "    model_path = os.path.join(model_dir, 'rein_model')\n",
    "    rein_model_path = model_path\n",
    "    print('Saving model...')\n",
    "    rein_model.save(model_path)\n",
    "    # tf.saved_model.save(rein_model, rein_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize(original, augmented):\n",
    "#   fig = plt.figure()\n",
    "#   plt.subplot(1, 2, 1)\n",
    "#   plt.title('Original image')\n",
    "#   plt.imshow(original)\n",
    "\n",
    "#   plt.subplot(1, 2, 2)\n",
    "#   plt.title('Augmented image')\n",
    "#   plt.imshow(augmented)\n",
    "\n",
    "\n",
    "# image, label = next(iter(train_dataset))\n",
    "# _ = plt.imshow(image)\n",
    "# _ = plt.title(get_label_name(label))\n",
    "\n",
    "# flipped = tf.image.flip_left_right(image)\n",
    "# visualize(image, flipped)\n",
    "\n",
    "# grayscaled = tf.image.rgb_to_grayscale(image)\n",
    "# visualize(image, tf.squeeze(grayscaled))\n",
    "# _ = plt.colorbar()\n",
    "\n",
    "\n",
    "# saturated = tf.image.adjust_saturation(image, 3)\n",
    "# visualize(image, saturated)\n",
    "\n",
    "\n",
    "# bright = tf.image.adjust_brightness(image, 0.4)\n",
    "# visualize(image, bright)\n",
    "\n",
    "\n",
    "# cropped = tf.image.central_crop(image, central_fraction=0.5)\n",
    "# visualize(image, cropped)\n",
    "\n",
    "# rotated = tf.image.rot90(image)\n",
    "# visualize(image, rotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test= 12630\n",
      "y_test= 12630\n",
      "<keras.engine.functional.Functional object at 0x0000022EE4C71BE0>\n",
      "test image share =  (1, 32, 32, 3)\n",
      "Predictions\n",
      "Image Prediction:  [[1.9978125e-02 9.7296178e-01 6.3240682e-03 1.5013407e-08 2.0886309e-05\n",
      "  7.1094016e-04 9.7204939e-08 2.9112025e-06 6.1639179e-07 7.2904148e-12\n",
      "  1.8937949e-14 4.2688279e-12 1.9731088e-13 2.6524874e-12 5.3649956e-11\n",
      "  1.2158712e-12 2.8118070e-14 2.2184184e-12 2.6156001e-13 2.0147017e-07\n",
      "  2.9318678e-15 3.9103010e-09 1.1852396e-11 1.6091366e-09 1.3173676e-10\n",
      "  2.1716145e-11 1.1978083e-13 5.3421656e-14 4.2607027e-08 1.5494572e-07\n",
      "  7.3969422e-14 8.3488902e-13 2.9043284e-15 1.4284083e-13 2.2801296e-11\n",
      "  1.6485384e-12 5.5648179e-09 4.3478327e-14 5.6917185e-14 2.0630503e-12\n",
      "  7.6949277e-08 2.5502750e-15 4.4594279e-14]]\n",
      "prediction.shape =  (1, 43)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# print(\"Reg Train Keras version: \", keras.__version__)\n",
    "# print(\"Reg Train TF version: \", tf.__version__)\n",
    "\n",
    "from ml_library.config import *\n",
    "from ml_library.utils import *\n",
    "from ml_library.model import *\n",
    "#from ml_library.w_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "testing_file = './Datasets/GTSRB_Final_Test_Images.zip'\n",
    "# testing_file = './Datasets/GTSRB_Online-Test-Images-Sorted.zip'\n",
    "test = generateTensor(testing_file, False)\n",
    "x_test, y_test = test['features'], test['labels']\n",
    "print(\"x_test=\", len(x_test))\n",
    "print(\"y_test=\", len(y_test))\n",
    "\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "#x_test, y_test = preprocess_data(x_test, y_test)\n",
    "#################\n",
    "\n",
    "if False:\n",
    "#def preprocess_data(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess image data, and convert labels into one-hot\n",
    "\n",
    "    Arguments:\n",
    "        * x: Image data\n",
    "        * y: Labels\n",
    "\n",
    "    Returns:\n",
    "        * Preprocessed x, one-hot version of y\n",
    "    \"\"\"\n",
    "    x = x_test\n",
    "    y = y_test\n",
    "# Convert from RGB to grayscale if applicable\n",
    "    if GRAYSCALE:\n",
    "        x = rgb_to_gray(x)\n",
    "\n",
    "    # Make all image array values fall within the range -1 to 1\n",
    "    # Note all values in original images are between 0 and 255, as uint8\n",
    "    x = x.astype('float32')\n",
    "    x = (x - 128.) / 128.\n",
    "\n",
    "    # Convert the labels from numerical labels to one-hot encoded labels\n",
    "    y_onehot = np.zeros((y.shape[0], NUM_CLASSES))\n",
    "    for i, onehot_label in enumerate(y_onehot):\n",
    "        onehot_label[y[i]] = 1.\n",
    "    y = y_onehot\n",
    "\n",
    "    x_test = x\n",
    "    y_test = y\n",
    "\n",
    "    #return x, y\n",
    "\n",
    "#################\n",
    "# print(\"x_test shape:\", x_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "model_dir = './models/'\n",
    "# normal_model_path = r\"C:\\Users\\akomm\\AppData\\Local\\Temp\\tmp4wcvqiyg\\normal_model\"\n",
    "#normal_model_path = r\"C:\\Users\\akomm\\AppData\\Local\\Temp\\tmp30k2qag3\\normal_model\"\n",
    "normal_model_path = os.path.join(model_dir, \"normal_model\")\n",
    "\n",
    "# # model_path = normal_model_path\n",
    "# model_path = rein_model_path\n",
    "# model = tf.saved_model.load(model_path)\n",
    "# adv_acc = model.evaluate()\n",
    "\n",
    "if True:    # no need of session in TF2\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    model_path = normal_model_path\n",
    "    # model_path = rein_model_path\n",
    "    #model = tf.saved_model.load(model_path)\n",
    "\n",
    "    # https://githubmemory.com/repo/tensorflow/decision-forests/issues/25\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(model)\n",
    "\n",
    "    # Prepare the validation dataset\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    # test_dataset = test_dataset.batch(64)\n",
    "    \n",
    "    #print(\"Evaluate\")\n",
    "    #acc = model.evaluate(test_dataset)\n",
    "\n",
    "    # image = x_test[np.random.randint(len(x_test))]\n",
    "    image = x_test[:3]\n",
    "\n",
    "    # Show original image for reference\n",
    "    # plt.subplot(3, 3, 1)\n",
    "    plt.imshow(image[1])\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    # for i in range(9):\n",
    "    #     image_x = transform_image(image, ANGLE, TRANSLATION, WARP)\n",
    "    #     plt.subplot(3, 3, i+2)\n",
    "    #     plt.imshow(image_x)\n",
    "    #     plt.title('Transformed Image %d' % (i+1,))\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    #image = x_test[1:2]\n",
    "    \n",
    "    #image1 = np.array(image[1])\n",
    "    #print(image1)\n",
    "    images_list = []\n",
    "    images_list.append(np.array(image[1]))\n",
    "    x = np.asarray(images_list)\n",
    "    print(\"test image shape = \", x.shape)\n",
    "    #pr_mask = model.predict(x).round()\n",
    "    #print(\"x = \", x)\n",
    "\n",
    "    #img = image[1]\n",
    "    print(\"Predictions\")\n",
    "    prediction = np.argmax(model.predict(x), axis=None, out=None)\n",
    "    print(\"Image Prediction: \", prediction)\n",
    "    print(\"prediction.shape = \", prediction.shape)\n",
    "    # prediction = model.predict(image_x)\n",
    "    # print(\"Image Prediction: \", prediction)\n",
    "    # print(\"Test Set Accuracy = {:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "    if False:\n",
    "        for i in range(20):\n",
    "            rain_images = rain(x_test, i)\n",
    "            adv_acc = model.evaluate(rain_images, y_test)\n",
    "            print(\"Rain Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "        \n",
    "        for i in range(20):\n",
    "            rain_images = fog(x_test, i)\n",
    "            adv_acc = model.evaluate(rain_images, y_test)\n",
    "            print(\"fog Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "        \n",
    "        for i in range(20):\n",
    "            rain_images = light(x_test, i)\n",
    "            adv_acc = model.evaluate(rain_images, y_test)\n",
    "            print(\"light Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" %\n",
    "                i, acc, adv_acc)\n",
    "        \n",
    "        for i in range(20):\n",
    "            rain_images = dark(x_test, i)\n",
    "            adv_acc = model.evaluate(rain_images, y_test)\n",
    "            print(\"dark Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "        \n",
    "        for i in range(20):\n",
    "            rain_images = blur(x_test, i)\n",
    "            adv_acc = model.evaluate(rain_images, y_test)\n",
    "            print(\"Blur deviation %.2d  vs. Accuracy vs Adv_Accuracy：\" %\n",
    "                i, acc, adv_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
